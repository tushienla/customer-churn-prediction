{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOMER CHURN PREDICTION\n",
    "\n",
    "This notebook outlines the steps taken to build a predictive model for identifying customers at high risk of churn for a telecom company. The dataset used is synthetically generated, with a focus on simulating real-world data challenges, including missing values, outliers, and imbalanced classes. The goal is to demonstrate the full machine learning lifecycle, from data generation and exploration to model building, evaluation, and (optionally) deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GENERATION\n",
    "\n",
    "In this section, we generate a synthetic dataset of 5000 customer records. The dataset includes various features like `CustomerID`, `Age`, `Gender`, `ContractType`, `MonthlyCharges`, `TotalCharges`, and others. Additionally, derived features like `average_monthly_charges` and `customer_lifetime_value` are created. We also introduce missing values and outliers to simulate real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of records\n",
    "n_records = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "data = pd.DataFrame({\n",
    "    'CustomerID': np.arange(1, n_records + 1),\n",
    "    'Age': np.random.randint(18, 70, size=n_records),\n",
    "    'Gender': np.random.choice(['Male', 'Female'], size=n_records),\n",
    "    'ContractType': np.random.choice(['Month-to-month', 'One year', 'Two year'], size=n_records),\n",
    "    'MonthlyCharges': np.round(np.random.uniform(20, 120, size=n_records), 2),\n",
    "    'TotalCharges': np.round(np.random.uniform(100, 8000, size=n_records), 2),\n",
    "    'TechSupport': np.random.choice(['Yes', 'No'], size=n_records),\n",
    "    'InternetService': np.random.choice(['DSL', 'Fiber optic', 'No'], size=n_records),\n",
    "    'Tenure': np.random.randint(1, 72, size=n_records),\n",
    "    'PaperlessBilling': np.random.choice(['Yes', 'No'], size=n_records),\n",
    "    'PaymentMethod': np.random.choice(['Cash','UPI','Internet Banking', 'Debit card', 'Credit card'], size=n_records),\n",
    "    'Churn': np.random.choice(['Yes', 'No'], size=n_records, p=[0.2, 0.8])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived features\n",
    "data['average_monthly_charges'] = data['TotalCharges'] / np.where(data['Tenure'] == 0, 1, data['Tenure'])\n",
    "data['customer_lifetime_value'] = data['MonthlyCharges'] * data['Tenure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce missing values\n",
    "data.loc[np.random.choice(data.index, size=50, replace=False), 'TotalCharges'] = np.nan\n",
    "\n",
    "# Introduce outliers\n",
    "data.loc[np.random.choice(data.index, size=10, replace=False), 'MonthlyCharges'] *= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to CSV\n",
    "data.to_csv('data/customer_data.csv', index=False)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "EDA is a crucial step in understanding the dataset characteristics, identifying patterns, and detecting anomalies. In this section, we perform in-depth exploratory data analysis, including summary statistics, distribution analysis, and visualization of relationships between features and the target variable `Churn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/customer_data.csv')\n",
    "\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze churn distribution\n",
    "sns.countplot(data['Churn'])\n",
    "plt.title('Churn Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numerical features\n",
    "sns.pairplot(data, hue='Churn', vars=['Age', 'MonthlyCharges', 'TotalCharges', 'Tenure'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "numeric_cols = data.select_dtypes(include=[np.number])\n",
    "\n",
    "corr_matrix = numeric_cols.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "for col in ['Gender', 'ContractType', 'TechSupport', 'InternetService', 'PaperlessBilling', 'PaymentMethod']:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(x=col, hue='Churn', data=data)\n",
    "    plt.title(f'{col} vs Churn')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING\n",
    "\n",
    "Data preprocessing involves cleaning and preparing the data for modeling. This section covers handling missing values, encoding categorical variables, scaling numerical features, and splitting the dataset into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['TotalCharges'] = imputer.fit_transform(data[['TotalCharges']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in ['Gender', 'ContractType', 'TechSupport', 'InternetService', 'PaperlessBilling', 'PaymentMethod', 'Churn']:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "data[['Age', 'MonthlyCharges', 'TotalCharges', 'Tenure', 'average_monthly_charges', 'customer_lifetime_value']] = \\\n",
    "    scaler.fit_transform(data[['Age', 'MonthlyCharges', 'TotalCharges', 'Tenure', 'average_monthly_charges', 'customer_lifetime_value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X = data.drop(columns=['CustomerID', 'Churn'])\n",
    "y = data['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING\n",
    "\n",
    "Feature engineering involves creating new features based on domain knowledge or insights from EDA. In this section, we create additional features that may improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feature engineering\n",
    "X_train['monthly_to_tenure_ratio'] = X_train['MonthlyCharges'] / np.where(X_train['Tenure'] == 0, 1, X_train['Tenure'])\n",
    "X_test['monthly_to_tenure_ratio'] = X_test['MonthlyCharges'] / np.where(X_test['Tenure'] == 0, 1, X_test['Tenure'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILDING\n",
    "\n",
    "In this section, we experiment with two classification algorithms, Decision Trees and Naive Bayes. We also optimize hyperparameters and evaluate model performance using metrics like accuracy, precision, recall, F1-score, and ROC AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE\n",
    "Decision trees are created by recursively partitioning the data into smaller and smaller subsets. At each partition, the data is split based on a specific feature, and the split is made in a way that maximizes the information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Decision Tree\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_tree):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_tree):.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_tree):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_tree):.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, tree.predict_proba(X_test)[:, 1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X and y are your features and target variables\n",
    "# Address class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the resampled dataset into training and testing sets\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=tree,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=10,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best estimator\n",
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_best_tree = best_tree.predict(X_test_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f\"Best Decision Tree Accuracy: {accuracy_score(y_test_smote, y_pred_best_tree):.2f}\")\n",
    "print(f\"Best Precision: {precision_score(y_test_smote, y_pred_best_tree):.2f}\")\n",
    "print(f\"Best Recall: {recall_score(y_test_smote, y_pred_best_tree):.2f}\")\n",
    "print(f\"Best F1-Score: {f1_score(y_test_smote, y_pred_best_tree):.2f}\")\n",
    "print(f\"Best ROC AUC: {roc_auc_score(y_test_smote, best_tree.predict_proba(X_test_smote)[:, 1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem, assuming independence between predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Naive Bayes\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.2f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_nb):.2f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_nb):.2f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_nb):.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, nb.predict_proba(X_test)[:, 1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline with scaling and the Naive Bayes model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Feature scaling\n",
    "    ('nb', GaussianNB())  # Step 2: Gaussian Naive Bayes model\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'nb__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]  # This is the only hyperparameter in GaussianNB\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',  # You can change this to 'f1' or another metric if needed\n",
    "                           cv=5,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best model\n",
    "best_nb = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_nb = best_nb.predict(X_test_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "print(f\"Best Naive Bayes Accuracy: {accuracy_score(y_test_smote, y_pred_best_nb):.2f}\")\n",
    "print(f\"Best Precision: {precision_score(y_test_smote, y_pred_best_nb):.2f}\")\n",
    "print(f\"Best Recall: {recall_score(y_test_smote, y_pred_best_nb):.2f}\")\n",
    "print(f\"Best F1-Score: {f1_score(y_test_smote, y_pred_best_nb):.2f}\")\n",
    "print(f\"Best ROC AUC: {roc_auc_score(y_test_smote, best_nb.predict_proba(X_test_smote)[:, 1]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SELECTION AND EVALUATION\n",
    "\n",
    "Based on the performance metrics, we select the best model, `best_tree`. In this section, we analyze the confusion matrix, calculate feature importance, and plot the ROC curve to further understand the model's behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Decision Tree\n",
    "conf_matrix_logreg = confusion_matrix(y_test_smote, y_pred_best_tree)\n",
    "sns.heatmap(conf_matrix_logreg, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for Decision Tree\n",
    "importances = pd.Series(tree.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values().plot(kind='barh', color='teal')\n",
    "plt.title('Decision Tree Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(model, X_test_smote, y_test_smote, model_name):\n",
    "    y_prob = model.predict_proba(X_test_smote)[:, 1] \n",
    "    fpr, tpr, thresholds = roc_curve(y_test_smote, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "# Plot ROC Curves for both models\n",
    "plot_roc(best_tree, X_test_smote, y_test_smote, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "\n",
    "In this project, we successfully built a predictive model for customer churn using a synthetically generated dataset. We followed a systematic approach, starting from data generation and exploratory analysis, through preprocessing and feature engineering, to model building and evaluation. The selected model `best_tree`, based on performance metrics, provides a robust solution for identifying high-risk customers, enabling the telecom company to take proactive measures to reduce churn.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
